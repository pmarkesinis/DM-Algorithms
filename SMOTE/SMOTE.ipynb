{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Mining Lab Assignment 1: SMOTE\n",
    "\n",
    "*Data Mining 2022/2023*  \n",
    "Created by Aleksander Buszydlik  \n",
    "Reviewed by Bianca Cosma\n",
    "\n",
    "**WHAT** This optional lab assignment covers the topics of **SMOTE** and **Tomek Links**. It consists of several programming exercises and insight questions.  \n",
    "\n",
    "**WHY** Practicing, both through programming and answering the insight questions, aims at deepening your knowledge and preparing you for the exam.  \n",
    "\n",
    "**HOW** Follow the exercises in this notebook either on your own or with a friend. Use Answers-EWI to discuss the theory questions with your peers. For feedback please consult the TAs during the assigned lab session. The answers to these exercises will not be provided.\n",
    "\n",
    "**Note:** If you encounter any issues with your notebook, please always use `Kernel` `->` `Restart & Run All` before asking a TA."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary\n",
    "\n",
    "Sometimes in data mining (and machine learning) tasks we may have to deal with heavily imbalanced datasets. In such cases, we generally expect our algorithms to perform worse. Fortunately, we can attempt to remove the imbalance from the dataset. In this assignment you will have an opportunity to experiment with two oversampling approaches and one undersampling approach.\n",
    "\n",
    "Let's start by loading the data.\n",
    "\n",
    "$\\textbf{Exercise 1}$: Run the code below to read the datasets from csv files, plot them side by side."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "balanced_data.csv not found.",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mFileNotFoundError\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[1], line 8\u001B[0m\n\u001B[0;32m      5\u001B[0m random_state \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m42\u001B[39m\n\u001B[0;32m      6\u001B[0m np\u001B[38;5;241m.\u001B[39mrandom\u001B[38;5;241m.\u001B[39mseed(random_state)\n\u001B[1;32m----> 8\u001B[0m balanced_data \u001B[38;5;241m=\u001B[39m \u001B[43mnp\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mloadtxt\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mbalanced_data.csv\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdelimiter\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43m,\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[0;32m      9\u001B[0m imbalanced_data \u001B[38;5;241m=\u001B[39m np\u001B[38;5;241m.\u001B[39mloadtxt(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mimbalanced_data.csv\u001B[39m\u001B[38;5;124m\"\u001B[39m, delimiter\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m,\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m     11\u001B[0m \u001B[38;5;66;03m# START ANSWER\u001B[39;00m\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\numpy\\lib\\npyio.py:1356\u001B[0m, in \u001B[0;36mloadtxt\u001B[1;34m(fname, dtype, comments, delimiter, converters, skiprows, usecols, unpack, ndmin, encoding, max_rows, quotechar, like)\u001B[0m\n\u001B[0;32m   1353\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(delimiter, \u001B[38;5;28mbytes\u001B[39m):\n\u001B[0;32m   1354\u001B[0m     delimiter \u001B[38;5;241m=\u001B[39m delimiter\u001B[38;5;241m.\u001B[39mdecode(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mlatin1\u001B[39m\u001B[38;5;124m'\u001B[39m)\n\u001B[1;32m-> 1356\u001B[0m arr \u001B[38;5;241m=\u001B[39m \u001B[43m_read\u001B[49m\u001B[43m(\u001B[49m\u001B[43mfname\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdtype\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mdtype\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcomment\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mcomment\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdelimiter\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mdelimiter\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1357\u001B[0m \u001B[43m            \u001B[49m\u001B[43mconverters\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mconverters\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mskiplines\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mskiprows\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43musecols\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43musecols\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1358\u001B[0m \u001B[43m            \u001B[49m\u001B[43munpack\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43munpack\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mndmin\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mndmin\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mencoding\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mencoding\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1359\u001B[0m \u001B[43m            \u001B[49m\u001B[43mmax_rows\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mmax_rows\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mquote\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mquotechar\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   1361\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m arr\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\numpy\\lib\\npyio.py:975\u001B[0m, in \u001B[0;36m_read\u001B[1;34m(fname, delimiter, comment, quote, imaginary_unit, usecols, skiplines, max_rows, converters, ndmin, unpack, dtype, encoding)\u001B[0m\n\u001B[0;32m    973\u001B[0m     fname \u001B[38;5;241m=\u001B[39m os\u001B[38;5;241m.\u001B[39mfspath(fname)\n\u001B[0;32m    974\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(fname, \u001B[38;5;28mstr\u001B[39m):\n\u001B[1;32m--> 975\u001B[0m     fh \u001B[38;5;241m=\u001B[39m \u001B[43mnp\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mlib\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_datasource\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mopen\u001B[49m\u001B[43m(\u001B[49m\u001B[43mfname\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mrt\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mencoding\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mencoding\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    976\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m encoding \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m    977\u001B[0m         encoding \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mgetattr\u001B[39m(fh, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mencoding\u001B[39m\u001B[38;5;124m'\u001B[39m, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mlatin1\u001B[39m\u001B[38;5;124m'\u001B[39m)\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\numpy\\lib\\_datasource.py:193\u001B[0m, in \u001B[0;36mopen\u001B[1;34m(path, mode, destpath, encoding, newline)\u001B[0m\n\u001B[0;32m    156\u001B[0m \u001B[38;5;250m\u001B[39m\u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[0;32m    157\u001B[0m \u001B[38;5;124;03mOpen `path` with `mode` and return the file object.\u001B[39;00m\n\u001B[0;32m    158\u001B[0m \n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    189\u001B[0m \n\u001B[0;32m    190\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[0;32m    192\u001B[0m ds \u001B[38;5;241m=\u001B[39m DataSource(destpath)\n\u001B[1;32m--> 193\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mds\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mopen\u001B[49m\u001B[43m(\u001B[49m\u001B[43mpath\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmode\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mencoding\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mencoding\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mnewline\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mnewline\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\numpy\\lib\\_datasource.py:533\u001B[0m, in \u001B[0;36mDataSource.open\u001B[1;34m(self, path, mode, encoding, newline)\u001B[0m\n\u001B[0;32m    530\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m _file_openers[ext](found, mode\u001B[38;5;241m=\u001B[39mmode,\n\u001B[0;32m    531\u001B[0m                               encoding\u001B[38;5;241m=\u001B[39mencoding, newline\u001B[38;5;241m=\u001B[39mnewline)\n\u001B[0;32m    532\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m--> 533\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mFileNotFoundError\u001B[39;00m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mpath\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m not found.\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
      "\u001B[1;31mFileNotFoundError\u001B[0m: balanced_data.csv not found."
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "random_state = 42\n",
    "np.random.seed(random_state)\n",
    "\n",
    "balanced_data = np.loadtxt(\"balanced_data.csv\", delimiter=\",\")\n",
    "imbalanced_data = np.loadtxt(\"imbalanced_data.csv\", delimiter=\",\")\n",
    "\n",
    "# START ANSWER\n",
    "print(balanced_data.shape)\n",
    "plt.scatter(balanced_data[:, 0], balanced_data[:, 1], c = balanced_data[:, 2])\n",
    "plt.show()\n",
    "plt.scatter(imbalanced_data[:, 0], imbalanced_data[:, 1], c = imbalanced_data[:, 2])\n",
    "plt.show()\n",
    "\n",
    "# END ANSWER"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\textbf{Exercise 2}$: Check what is the number of positive and negative samples in each of the datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "positive_balanced = np.sum(balanced_data[:, 2] == 1.0)\n",
    "negative_balanced = np.sum(balanced_data[:, 2] == 0.0)\n",
    "        \n",
    "positive_imbalanced = np.sum(imbalanced_data[:, 2] == 1.0)\n",
    "negative_imbalanced = np.sum(imbalanced_data[:, 2] == 0.0)\n",
    "        \n",
    "plt.bar([\"Positive class\", \"Negative class\"], [positive_balanced, negative_balanced])\n",
    "plt.show()\n",
    "plt.bar([\"Positive class\", \"Negative class\"], [positive_imbalanced, negative_imbalanced])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "A note on randomness in Python:\n",
    "Some functions (for instance those from the numpy.random library) generate non-deterministic results.\n",
    "We can control the randomness to some degree by setting a random seed. In a nutshell, whenever a function using\n",
    "the random seed is called, the seed will be incremented. However, this makes it difficult to provide you with \n",
    "stable assertions to test your code. For instance, if our assertion is based on a function call that\n",
    "increments the random seed two times, and you introduce one more random operation, the results may be different.\n",
    "In this lab you will make use of two functions from numpy.random. To help you test your code, we make a small\n",
    "wrapper around them which sets the random seed just before the function is called. If you would like to make sure that \n",
    "our assertions work, use the functions below instead of calling numpy.random directly.\n",
    "\"\"\"\n",
    "\n",
    "def choice(a, size=None, replace=True, p=None):\n",
    "    np.random.seed(random_state)\n",
    "    return np.random.choice(a, size, replace, p)\n",
    "\n",
    "def uniform(low=0.0, high=1.0, size=None):\n",
    "    np.random.seed(random_state)\n",
    "    return np.random.uniform(low, high, size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also need a way to assess the performance of a classifier on each dataset. In this lab exercise we will use a `LogisticRegression` classifier from the `scikit-learn` library with default hyper-parameters. We will assess the performance of the classifier using three metrics: the _accuracy_, the _True Positive Rate_ (_TPR_), and the _False Positive Rate_ (_FPR_).\n",
    "\n",
    "$\\textbf{Exercise 3}$: Finish the method `evaluate` according to the provided docstring."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, balanced_accuracy_score\n",
    "\n",
    "def evaluate(data):\n",
    "    \"\"\"\n",
    "    Split the provided `data` into 70% training set and 30% test set, fit a LogisticRegression classifier,\n",
    "    and evaluate the accuracy, TPR, and FPR on its predictions for the test set.\n",
    "    \n",
    "    Args\n",
    "        data (numpy.ndarray): Dataset containing the features along with their corresponding labels.\n",
    "        \n",
    "    Returns\n",
    "        (float, float, float): Calculated accuracy, True Positive Rate, and False Positive Rate\n",
    "    \"\"\"\n",
    "    \n",
    "    np.random.seed(random_state)\n",
    "    X_train = X_test = y_train = y_test = None\n",
    "    tpr = fpr = acc = 0\n",
    "    \n",
    "    \n",
    "    # START ANSWER\n",
    "    X_train,  y_train, X_test, y_test = train_test_split(data[:, :2], data[:, 2], test_size = 0.3, random_state = random_state)\n",
    "    model = LogisticRegression(random_state = random_state).fit(X_train, X_test)\n",
    "    acc = model.score(y_train, y_test)\n",
    "    predict = model.predict(y_train)\n",
    "    confusion = confusion_matrix(y_test, predict)\n",
    "    tpr = confusion[1,1] / np.sum(confusion[1, :])\n",
    "    fpr = confusion[0,1] / np.sum(confusion[0, :])\n",
    "    # END ANSWER\n",
    "    \n",
    "    print(\"Accuracy: {} -- TPR: {} -- FPR: {}\".format(acc, tpr, fpr))   \n",
    "    \n",
    "    return acc, tpr, fpr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "balanced_acc, balanced_tpr, balanced_fpr = evaluate(balanced_data)\n",
    "imbalanced_acc, imbalanced_tpr, imbalanced_fpr = evaluate(imbalanced_data)\n",
    "\n",
    "assert np.isclose(balanced_acc, 0.908, atol=1e-03)\n",
    "assert np.isclose(balanced_tpr, 0.914, atol=1e-03)\n",
    "assert np.isclose(balanced_fpr, 0.097, atol=1e-03)\n",
    "assert np.isclose(imbalanced_acc, 0.970, atol=1e-03)\n",
    "assert np.isclose(imbalanced_tpr, 1.000, atol=1e-03)\n",
    "assert np.isclose(imbalanced_fpr, 0.333, atol=1e-03)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\textbf{Question 1}$: If we only consider the accuracy scores, it seems like the classifier trained on the imbalanced dataset actually performs better than its balanced counterpart. Why is that not actually the case?\n",
    "\n",
    "$\\textbf{Question 2}$: Give examples of three domains (scenarios) where we are likely to encounter heavily imbalanced data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Oversampling with replacement\n",
    "\n",
    "First, we will see if the naive approach of oversampling the minority class works well. To that end, we would like to sample with replacement from the existing minority instances until we arrive at the same number of datapoints in both classes.\n",
    "\n",
    "$\\textbf{Exercise 4}$: Finish the method `oversample_with_replacement` below.  \n",
    "**Hint:** You can use `np.random.choice()` to sample from an array with replacement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def oversample_with_replacement(data, majority_label=1, minority_label=0):\n",
    "    \"\"\"\n",
    "    Augment the dataset with samples (with replacement) from the minority class until the two classes are equally large.\n",
    "    \n",
    "    Args\n",
    "        data (numpy.ndarray): Dataset containing the features along with their corresponding labels.\n",
    "        majority_label (int): Label of the majority class in the dataset.\n",
    "        minority_label (int): Label of the minority class in the dataset.\n",
    "        \n",
    "    Returns\n",
    "        (numpy.ndarray): Balanced version of the initial dataset.\n",
    "    \"\"\"\n",
    "    \n",
    "    random.seed(random_state)\n",
    "    np.random.seed(random_state)\n",
    "    \n",
    "    majority_samples = data[np.where(data[:, 2] == majority_label)]\n",
    "    minority_samples = data[np.where(data[:, 2] == minority_label)]\n",
    "    \n",
    "    generated_samples = required_samples = None\n",
    "    # Calculate the number of required instances and generate them with replacement\n",
    "    # START ANSWER\n",
    "    required_samples = len(majority_samples) - len(minority_samples)\n",
    "    generated_samples = [minority_samples[i] for i in choice(len(minority_samples), size = required_samples)]\n",
    "    # END ANSWER\n",
    "    return np.r_[majority_samples, minority_samples, generated_samples]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from copy import deepcopy\n",
    "\n",
    "oversampled_data = oversample_with_replacement(deepcopy(imbalanced_data))\n",
    "\n",
    "assert len(oversampled_data[np.where(oversampled_data[:, 2] == 0)]) == len(oversampled_data[np.where(oversampled_data[:, 2] == 1)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\textbf{Question 3}$: What is the main disadvantage of this sampling strategy?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\textbf{Exercise 5}$: Train the `LogisticRegression` classifier on the oversampled data and plot the instances."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# START ANSWER\n",
    "plt.scatter(oversampled_data[:, 0], oversampled_data[:, 1], c = oversampled_data[:, 2])\n",
    "plt.show()\n",
    "new_acc, new_tpr, new_fpr = evaluate(oversampled_data)\n",
    "# END ANSWER"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\textbf{Question 4}$: How did the performance of the classifier change compared to the imbalanced version? Why?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SMOTE\n",
    "\n",
    "SMOTE (Synthetic Minority Oversampling Technique) is a popular method for generating artificial data samples. We can apply SMOTE when we are working with a dataset that is severely imbalanced. Instead of repeatedly sampling (with or without replacement) the existing data points, we can adapt a more systematic approach which allows us to generate new data points with similar characteristics to the existing instances of the minority class. As a brief recap, the technique works as follows:\n",
    "\n",
    "1. Determine _k_ nearest neighbors of every instance in the minority class.\n",
    "2. Randomly select a fraction of the _k_ nearest neighbors found for each instance from the minority class.\n",
    "3. Generate a new data point between every minority instance and each of its selected _k_ nearest neighbors.\n",
    "\n",
    "We will implement SMOTE to oversample the dataset in a more sophisticated way. We need a few helper functions first.\n",
    "\n",
    "$\\textbf{Exercise 6}$: Complete the functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def distance(a, b):\n",
    "    \"\"\"\n",
    "    Calculate Euclidean distance between two points.\n",
    "    \n",
    "    Args\n",
    "        a (numpy.ndarray): Vector representing a point in the classification space.\n",
    "        b (numpy.ndarray): Vector representing a point in the classification space.\n",
    "        \n",
    "    Returns\n",
    "        float: Euclidean distance between points `a` and `b`.\n",
    "    \"\"\"\n",
    "    d = 0\n",
    "    \n",
    "    # START ANSWER\n",
    "    d = np.linalg.norm(a-b)\n",
    "    # END ANSWER\n",
    "    return d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def random_lerp(a, b):\n",
    "    \"\"\"\n",
    "    Find a random point on the line between `a` and `b` using linear interpolation.\n",
    "    \n",
    "    Args\n",
    "        a (numpy.ndarray): Vector representing a point in the classification space.\n",
    "        b (numpy.ndarray): Vector representing a point in the classification space.\n",
    "        \n",
    "    Returns\n",
    "        numpy.ndarray: A random point between `a` and `b`.\n",
    "    \"\"\"\n",
    "    random_position = uniform(size=1)\n",
    "    point = None\n",
    "    \n",
    "    # START ANSWER\n",
    "    point = random_position*a + (1-random_position)*b\n",
    "    # END ANSWER\n",
    "    \n",
    "    return point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def nearest_neighbors(test_samples, data, k):\n",
    "    \"\"\"\n",
    "    Find the `k` nearest neighbors for each of the test samples\n",
    "    \n",
    "    Args\n",
    "        test_samples (numpy.ndarray): Selected set of samples from the dataset, for example minority or majority class.\n",
    "        data (numpy.ndarray): Dataset where the nearest neighbors of `test_samples` should be found.\n",
    "        k (int): Number of nearest neighbors that should be returned for each of the `test_samples`.\n",
    "        \n",
    "    Returns\n",
    "        numpy.ndarray: Array of size `test_samples` x `k` with `k` nearest neighbors for each test sample\n",
    "    \"\"\"\n",
    "    \n",
    "    neighbors = []\n",
    "    for i, sample in enumerate(test_samples):\n",
    "        \n",
    "        distances = []\n",
    "        # Find the `k` nearest neighbors of the `sample` within `data`\n",
    "        # Hint: represent the neighbors as tuples in the form (index_of_datapoint, distance)\n",
    "        # START ANSWER\n",
    "        distances = [(index, distance(sample, j)) for index, j in enumerate(data) if np.sum(j != sample) != 0]\n",
    "        # END ANSWER\n",
    "        \n",
    "        # If you use a different representation of the nearest neighbors, you may need to change the next 3 lines\n",
    "        distances = sorted(distances, key=lambda t: t[1])\n",
    "        distances = list(map(lambda t: t[0], distances))\n",
    "        neighbors.append(np.array(distances[:k]))\n",
    "    return np.array(neighbors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# You can run this cell to test your code\n",
    "test_data = np.array([[1.0, 1.0], [1.0, 2.0], [2.0, 3.0], [2.5, 3.0], [4.0, 4.0]])\n",
    "assert np.isclose(distance(test_data[0], test_data[2]), 2.236, atol=1e-03)\n",
    "assert np.isclose(distance(test_data[0], test_data[3]), 2.500, atol=1e-03)\n",
    "assert np.array_equal(nearest_neighbors(test_data, test_data, 2), np.array([[1, 2], [0, 2], [3, 1], [2, 1], [3, 2]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\textbf{Exercise 7}$: Implement the complete SMOTE algorithm using your helper functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def SMOTE(data, sampling_level, minority_label=0, k=3):\n",
    "    \"\"\"\n",
    "    Applies SMOTE to generate a set of new instances from the minority class using nearest neighbors of existing observations.\n",
    "    \n",
    "    Args\n",
    "        data (numpy.ndarray): Dataset containing the features along with their corresponding labels.\n",
    "        sampling_level (float): Fraction of the NNs of each instance that should be used to generate new instances.\n",
    "        minority_label (int): Label of the minority class in the dataset.\n",
    "        k (int): Number of nearest neighbors that should be found for each of the minority instances.\n",
    "        \n",
    "    Returns\n",
    "        numpy.ndarray: Array of newly generated minority instances.\n",
    "    \"\"\"\n",
    "    sample_size = int(np.ceil(k * sampling_level))\n",
    "    minority_samples = data[np.where(data[:, 2] == minority_label)][:, :2]\n",
    "    \n",
    "    sample_neighbors = None\n",
    "    \n",
    "    # Find the `k` nearest neighbors for each sample in `class_samples`\n",
    "    # START ANSWER\n",
    "    sample_neighbors = nearest_neighbors(minority_samples, minority_samples, k)\n",
    "    # END ANSWER\n",
    "    \n",
    "    generated_samples = []\n",
    "    # Iterate through all samples of the minority class\n",
    "    for i, sample in enumerate(minority_samples):\n",
    "        selected_neighbors = []\n",
    "        \n",
    "        # Randomly (without replacement) select a subset of neighbors\n",
    "        # START ANSWER\n",
    "        selected_neighbours = choice(sample_neighbors[i], replace=False, size = sample_size)\n",
    "        # END ANSWER\n",
    "        \n",
    "        # For each of the `selected_neighbors` generate a point between itself and the original sample\n",
    "        # START ANSWER\n",
    "        generated_samples.extend([random_lerp(minority_samples[j], sample) for j in selected_neighbours])\n",
    "        # END ANSWER\n",
    "    \n",
    "    # Add labels to the newly generated datapoints\n",
    "    labels = np.full(len(generated_samples), minority_label).reshape(-1, 1)\n",
    "    generated_samples = np.array(generated_samples)\n",
    "    generated_samples = np.c_[generated_samples, labels]\n",
    "            \n",
    "    return generated_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_samples = SMOTE(imbalanced_data, 0.6, k=15)\n",
    "smote_data = np.r_[imbalanced_data, new_samples]\n",
    "\n",
    "assert len(smote_data) == 400"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\textbf{Exercise 8}$: Train the `LogisticRegression` classifier on the SMOTE data and plot the instances."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# START ANSWER\n",
    "plt.scatter(smote_data[:, 0], smote_data[:, 1], c = smote_data[:, 2])\n",
    "plt.show()\n",
    "smote_acc, smote_tpr, smote_fpr = evaluate(smote_data)\n",
    "# END ANSWER"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\textbf{Question 5}$: How did the performance of the classifier change compared to the previous versions?  \n",
    "\n",
    "$\\textbf{Question 6}$: What are the characteristics of the distribution generated by SMOTE? Does it resemble the \"actual\" distribution in the balanced dataset?  \n",
    "\n",
    "$\\textbf{Question 7}$: Can you imagine any situations where SMOTE would work poorly?  \n",
    "\n",
    "$\\textbf{Question 8}$: SMOTE generates new instances along the lines connecting all pairs of minority class samples. When the number of existing samples is low, this can lead to \"blocky\" distributions. Can you think of a way to adapt SMOTE so that it generates distributions that look more naturally?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tomek Links\n",
    "\n",
    "Finally, we will consider one more approach to addressing class imbalance. We will remove some datapoints $p_{i}$ that satisfy the following requirements:\n",
    "\n",
    "1. $p_{i}$ belongs to the majority class\n",
    "2. $n_{i}$ – the nearest neighbor of $p_{i}$ – belongs to the minority class.\n",
    "3. $p_{i}$ is also the nearest neighbor of $n_{i}$\n",
    "\n",
    "Points which satisfy this set of requirements are referred to as _Tomek Links_. Of course, we do not necessarily want to remove all Tomek Links from the dataset. Thus, we will specify a removal level and choose the number of instances from the majority class which will be tested based on this removal level.\n",
    "\n",
    "$\\textbf{Exercise 9}$: Implement the function `find_tomek_links`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def find_tomek_links(data, removal_level, majority_label=1):\n",
    "    \"\"\"\n",
    "    Test a random subset of points from the majority class for being Tomek Links and return them.\n",
    "    \n",
    "    Args\n",
    "        data (numpy.ndarray): Dataset containing the features along with their corresponding labels.\n",
    "        removal_level (float): Fraction of the majority class which should be tested for satisfying the properties.\n",
    "        majority_label (int): Label of the majority class in the dataset.\n",
    "        \n",
    "    Returns\n",
    "        numpy.ndarray: Tomek Links identified in the original dataset.\n",
    "    \"\"\"\n",
    "\n",
    "    X = data[:, :2]\n",
    "    y = data[:, 2]\n",
    "    \n",
    "    # We will find the 1-NN for every point in the dataset\n",
    "    neighbors = nearest_neighbors(X, X, 1)  \n",
    "    tomek_links = []\n",
    "    remove_limit = int(np.ceil(removal_level * len(np.where(data[:, 2] == majority_label))))\n",
    "    # START ANSWER\n",
    "    majority_class = np.array([i for i,x in enumerate(data) if x[2] == majority_label])\n",
    "    majority_removed = choice(majority_class, size = int(np.ceil(len(majority_class) * removal_level)), replace= False)\n",
    "    tomek_links = [i for i in majority_removed if y[neighbors[i]] != majority_label and neighbors[neighbors[i]]==i]\n",
    "    # END ANSWER\n",
    "    print(tomek_links)\n",
    "    return np.array(tomek_links)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tomek_links = find_tomek_links(smote_data, 0.1)\n",
    "tomek_data = np.delete(smote_data, tomek_links, axis=0)\n",
    "assert len(tomek_links) == 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\textbf{Exercise 10}$: Train the `LogisticRegression` classifier one last time and plot the instances. Also plot the Tomek Links which were found by your method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# START ANSWER\n",
    "plt.scatter(smote_data[:, 0], smote_data[:, 1], c = smote_data[:, 2])\n",
    "smote_acc, smote_tpr, smote_fpr = evaluate(tomek_data)\n",
    "plt.scatter([smote_data[i, 0] for i in tomek_links], [smote_data[i, 1] for i in tomek_links])\n",
    "plt.show()\n",
    "# END ANSWER"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\textbf{Question 9}$: Did removing Tomek Links further improve the performance?\n",
    "\n",
    "$\\textbf{Question 10}$: Why do you think removing only a single point may have such impact on the performance?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 [3.7]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  },
  "vscode": {
   "interpreter": {
    "hash": "2469a70536e4d2335a2ea8907942d0699c37342a371ac185bdb5b0aa6f073890"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
